{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2256cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
      "Building wheels for collected packages: pypdf2\n",
      "  Building wheel for pypdf2 (setup.py): started\n",
      "  Building wheel for pypdf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for pypdf2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61101 sha256=c13ebe9ce15a065cc4e93f8468c34bbc2beae56f278ce30913122b1f3aec7c4e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\d9\\dc\\ec\\72da68331f30074b9950c1737c23cb8a67484e61498bc9713d\n",
      "Successfully built pypdf2\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c820a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filetype\n",
      "  Downloading filetype-1.0.10-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: filetype\n",
      "Successfully installed filetype-1.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbf35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f793e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 611: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23220/1517617915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".docx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mln\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\encodings\\cp1254.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 611: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import os\n",
    "\n",
    "name = [] \n",
    "ln = []\n",
    "  \n",
    "for filename in os.scandir(r\"C:\\\\Users\\User\\Downloads\\same-resume-year-wise-master\"):\n",
    "    if filename.path.endswith(\".pdf\"):\n",
    "        f = open(filename, 'rb')\n",
    "        data = f. readlines()\n",
    "        name.append(filename )\n",
    "        ln.append(len(data))\n",
    "name_len = zip(name, ln) ###at least two completed list are required to use zip\n",
    "list(name_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dfaf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, os, filetype\n",
    "def pdf_reader(dir_path, pdf_name):\n",
    "    \"\"\"\n",
    "    This function reads a complete PDf file \n",
    "    \"\"\"\n",
    "    pdf_path = os.path.join(dir_path,pdf_name)\n",
    "    if os.path.exists(pdf_path) and filetype.guess(pdf_path).mime == 'application/pdf':\n",
    "        pdfFileObject = open(pdf_path, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "        text=''\n",
    "        for i in range(0,pdfReader.numPages):\n",
    "            # creating a page object\n",
    "            pageObj = pdfReader.getPage(i)\n",
    "            # extracting text from page\n",
    "            text=text+pageObj.extractText()\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Please enter correct path as an argument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde409e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d='C:\\\\Users\\\\User\\\\Downloads\\\\same-resume-year-wise-master'\n",
    "f='6+.pdf'\n",
    "pdf_reader(d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac3a60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Downloads\\\\same-resume-year-wise-master'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8522858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_list(path):\n",
    "    files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "    files = filter(lambda f: f.endswith(('.pdf','.docx')), files)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de5ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = file_list(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6e5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfiles = list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db575c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12+ (2).docx',\n",
       " '12+.docx',\n",
       " '15+ (1).pdf',\n",
       " '15+.pdf',\n",
       " '20.pdf',\n",
       " '3+ (2).docx',\n",
       " '3+.docx',\n",
       " '3+.pdf',\n",
       " '4+.docx',\n",
       " '5+ .pdf',\n",
       " '5+.pdf',\n",
       " '5.pdf',\n",
       " '6+.pdf',\n",
       " '7+.pdf',\n",
       " '8+.docx',\n",
       " '8+.pdf',\n",
       " 'freasher .pdf',\n",
       " 'mteh fresher.pdf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f123075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edfa5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tika) (58.0.4)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from tika) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->tika) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->tika) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->tika) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->tika) (2.0.4)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=4abea89d49a1b0b9c906cf83f3e5ec6a3a21306d523fcdff789f8d71d086e11f\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\ec\\76\\38\\0e4b92d8a3a89cbfff5be03a40c02d15b2072b1b08ebf28d6a\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "207ffd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.19.6-cp39-cp39-win_amd64.whl (6.4 MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.19.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8fca0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while\\nbeing resourceful, innovative and flexible.\\nWORK EXPERIENCE\\nData Scientist\\nElegant Microweb Technologies Private Limited -  Ahmedabad, Gujarat -\\nMarch 2017 to Present\\nAhmedabad\\nJob Responsibilities:\\n• Develop statistical models for various predictive methods such as forecasting, classification,\\nclustering and regression.\\n• Analyze large datasets to provide strategic direction to clients for their business.\\n• Handling client issues related to ElegantJ BI tool (Plug &amp; play Predictive).\\n• Generating actionable insights from client data and creating presentations and dashboards to\\nmake recommendations for improvement.\\n• Scripting using R language as well as Apache Spark + Java for predictive algorithms such as\\nforecasting, classification, clustering, association mining, regression, decision tree, correlation.\\n• Automating &amp; integrating Predictive algorithms - R scripts in Plug &amp; play Predictive\\nAnalytics,\\nmodule of ElegantJ BI.\\n• Automating &amp; integrating Predictive algorithms - Spark scripts in Smarten, module of\\nElegantJ\\nBI.\\n• Preparing and conducting demonstration of predictive analytics module of BI along with\\nmarketing team.\\nWeb Analyst\\nTatvic Analytics Private Limited -  Ahmedabad, Gujarat -\\nMay 2016 to December 2016\\nAhmedabad\\nJob Responsibilities:\\n• Perform research and carry out new statistical analysis in R Language.\\n• Handling Google BigQuery related issues of clients.\\n• Generating actionable insights from client data and creating monthly reports, presentations\\nand dashboards to make recommendations for improvement.\\n• Solving client issues related to Google Analytics website and application data.\\nSeeking a position to utilize my skills and abilities in the Industry that offers professional growth\\n• Performing analysis namely Forecasting Customer Lifetime Value, Measuring Long-term Value\\nof Campaigns, Churn Analysis, Product Banner/Brand Sequence Analysis, Recency Frequency\\nMonetary Analysis, Sentiment Analysis, Navigation Menu Analysis, Net Promoter Score\\nAnalysis, Market Basket Analysis, TV Advertisement Analysis, Site Search Analysis, Marketing\\nChannel Attribution Analysis (Markov Chain Analysis), Related Products Analysis, Time Series\\nDecomposition, Product Demand Index Analysis, Scroll Depth Tracking Analysis, Anomaly\\nDetection Analysis, Causal Impact Analysis\\n• Working on Machine Learning and Predictive Analytics using R Language &amp; Google\\nPrediction\\nAPI.\\n• Blog Writing.\\nEDUCATION\\nM.Tech\\nComputer\\n2016\\nB.E. in Computer\\nSaurashtra University\\n2007\\nSKILLS\\nALGORITHMS (1 year), APACHE (1 year), JAVA (1 year), SCRIPTING (1 year), API (Less than 1\\nyear)\\nADDITIONAL INFORMATION\\nCORE COMPETENCIES\\n• Strategic Thinking.\\n• Expertise in data science softwares such as R, Python &amp;Apache Spark.\\n• Google Analytics &amp; Google BigQuery Consultant.\\n• Statistical Modelling &amp; analytic ability.\\n• Big data analytics.\\n• Machine learning &amp; Predictive algorithms.\\n• Communication &amp; Project Management.\\n• C#/Java, database and web technologies.\\nTECHNICAL SKILLS\\nOperating System Windows, Unix, Linux.\\nProgramming Language R, Python, SQL, C#.NET, Visual Basic.NET, Java.\\nDatabase MS Excel, MS Access, Oracle, SQL Server 2000.\\nMark Up / Scripting\\nHTML, ASP, ASP.NET, Core Java &amp; Advanced Java\\nLanguage/Web\\nProgramming.\\nTechnologies\\nPackages/Tools Apache Spark, Google Analytics, Google BigQuery, Google\\nPrediction API, R Studio, Rattle, Shiny, XLMiner, Weka, Rapid\\nMiner, Tanagra.\\nTheories Data Mining, Object Oriented Analysis &amp; Design, Management\\nInformation system, Data Structure &amp; Algorithms, Advanced\\nDatabase Management System, Operating Systems, System\\nAnalysis and Design &amp; Unified Modelling Language, Software\\nEngineering, .NET Platform, Technical writing.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "import itertools # this is pymupdf\n",
    "content = {}\n",
    "for file in myfiles:\n",
    "    if file.endswith(\".pdf\"):\n",
    "        with fitz.open(file) as doc:\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.getText()\n",
    "    content[file] = text\n",
    "content[\"3+.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7cbdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while\\nbeing resourceful, innovative and flexible.\\nWORK EXPERIENCE\\nData Scientist\\nElegant Microweb Technologies Private Limited -  Ahmedabad, Gujarat -\\nMarch 2017 to Present\\nAhmedabad\\nJob Responsibilities:\\n• Develop statistical models for various predictive methods such as forecasting, classification,\\nclustering and regression.\\n• Analyze large datasets to provide strategic direction to clients for their business.\\n• Handling client issues related to ElegantJ BI tool (Plug &amp; play Predictive).\\n• Generating actionable insights from client data and creating presentations and dashboards to\\nmake recommendations for improvement.\\n• Scripting using R language as well as Apache Spark + Java for predictive algorithms such as\\nforecasting, classification, clustering, association mining, regression, decision tree, correlation.\\n• Automating &amp; integrating Predictive algorithms - R scripts in Plug &amp; play Predictive\\nAnalytics,\\nmodule of ElegantJ BI.\\n• Automating &amp; integrating Predictive algorithms - Spark scripts in Smarten, module of\\nElegantJ\\nBI.\\n• Preparing and conducting demonstration of predictive analytics module of BI along with\\nmarketing team.\\nWeb Analyst\\nTatvic Analytics Private Limited -  Ahmedabad, Gujarat -\\nMay 2016 to December 2016\\nAhmedabad\\nJob Responsibilities:\\n• Perform research and carry out new statistical analysis in R Language.\\n• Handling Google BigQuery related issues of clients.\\n• Generating actionable insights from client data and creating monthly reports, presentations\\nand dashboards to make recommendations for improvement.\\n• Solving client issues related to Google Analytics website and application data.\\nSeeking a position to utilize my skills and abilities in the Industry that offers professional growth\\n• Performing analysis namely Forecasting Customer Lifetime Value, Measuring Long-term Value\\nof Campaigns, Churn Analysis, Product Banner/Brand Sequence Analysis, Recency Frequency\\nMonetary Analysis, Sentiment Analysis, Navigation Menu Analysis, Net Promoter Score\\nAnalysis, Market Basket Analysis, TV Advertisement Analysis, Site Search Analysis, Marketing\\nChannel Attribution Analysis (Markov Chain Analysis), Related Products Analysis, Time Series\\nDecomposition, Product Demand Index Analysis, Scroll Depth Tracking Analysis, Anomaly\\nDetection Analysis, Causal Impact Analysis\\n• Working on Machine Learning and Predictive Analytics using R Language &amp; Google\\nPrediction\\nAPI.\\n• Blog Writing.\\nEDUCATION\\nM.Tech\\nComputer\\n2016\\nB.E. in Computer\\nSaurashtra University\\n2007\\nSKILLS\\nALGORITHMS (1 year), APACHE (1 year), JAVA (1 year), SCRIPTING (1 year), API (Less than 1\\nyear)\\nADDITIONAL INFORMATION\\nCORE COMPETENCIES\\n• Strategic Thinking.\\n• Expertise in data science softwares such as R, Python &amp;Apache Spark.\\n• Google Analytics &amp; Google BigQuery Consultant.\\n• Statistical Modelling &amp; analytic ability.\\n• Big data analytics.\\n• Machine learning &amp; Predictive algorithms.\\n• Communication &amp; Project Management.\\n• C#/Java, database and web technologies.\\nTECHNICAL SKILLS\\nOperating System Windows, Unix, Linux.\\nProgramming Language R, Python, SQL, C#.NET, Visual Basic.NET, Java.\\nDatabase MS Excel, MS Access, Oracle, SQL Server 2000.\\nMark Up / Scripting\\nHTML, ASP, ASP.NET, Core Java &amp; Advanced Java\\nLanguage/Web\\nProgramming.\\nTechnologies\\nPackages/Tools Apache Spark, Google Analytics, Google BigQuery, Google\\nPrediction API, R Studio, Rattle, Shiny, XLMiner, Weka, Rapid\\nMiner, Tanagra.\\nTheories Data Mining, Object Oriented Analysis &amp; Design, Management\\nInformation system, Data Structure &amp; Algorithms, Advanced\\nDatabase Management System, Operating Systems, System\\nAnalysis and Design &amp; Unified Modelling Language, Software\\nEngineering, .NET Platform, Technical writing.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fitz.open(\"3+.pdf\") as doc:\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d19d250a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23220/1175647003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "list(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ebd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
